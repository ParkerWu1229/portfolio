---
title: "Web Crawler (Parse Data From Food Hygiene Agency)"
author: "Parker Wu"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
```

```{r}
library(rvest)
library(httr) # work with URLs and HTTP
library(dplyr)
library(stringr)
library(XML)
library(purrr)
library(readr)
library(data.table)
```
```{r}
# identify the url link that we want to use
url_to_fetch <- "https://data.food.gov.uk/catalog/datasets/38dd8d6a-5ab1-4f50-b753-ab33288e3200"
page_source <- read_html(url_to_fetch)
```

```{r}
# get all url links of each city
url.list <- page_source %>%
  html_nodes(".o-dataset-distribution--link") %>%
  html_attr("href")

# get all titles of each city
title.list <- page_source %>%
  html_nodes(".o-dataset-distribution--link") %>%
  html_attr("title")

# create a dataframe
download <- data.frame(title.list,url.list)

# cleaning data by removing non-related row and row with another language as we need to prepare the data for further analysis
download <- download[-1,]
download <- download[!str_detect(download$title.list,"Welsh language"), ]
```

```{r}
# create a local file to store all XML file
dir.create("XML/")

# create for loop to download file into the local file we created
for (i in 1:nrow(download)) {
  file <- download$url.list[i]
  filename <- download$title.list[i]
    download.file(url = file,
                destfile = paste0("XML/", filename))
}
```

```{r}
# create object keeping path for each file
data.path <- "XML/"

# create an empthy list for a final consolidation
establishment.list <- list()

# create for loop for extracting XML file
for (i in 1:length(list.files(data.path))) {
  
  # create a list containing each file
  list.files(data.path)[i] %>%
  
  # get full path of each file by paste0  
  paste0(data.path,.) %>% 
  
  # read each XML file and store in one local object
  xmlToList(.) -> local.xml
  
  # select establishment collection for each city preparing for another loop
  city.establishment.list <- local.xml$EstablishmentCollection
  
  # loop for combining all restaurants detail for each city
  for (n in 1:length(city.establishment.list)) {
    restaurant.list <- (city.establishment.list[n]$EstablishmentDetail) %>%
      # flatten the result to be a one-level list (flatten() is similar to unlist(), but return a result as list instead of vector)
      flatten()
   
    # gather each restaurant together into one large list containing all restaurants information
   establishment.list[[length(establishment.list)+1]] <- restaurant.list
    }
}

# convert all list into interpretable format using rbindlist
final.data <- rbindlist(establishment.list, fill = TRUE)

```

```{r}
# export CSV file
write.csv(final.data,'total_restaurants.data.csv', row.names = FALSE)
```
```{r}
# save file as the rds file
saveRDS(final.data, "total_restaurants.rds")
```